# ðŸš¨ CRITICAL: Fix for Low IoU Performance (0.021)

## Problem Summary

Your model trained for 100 epochs achieved:
- **Overall Accuracy**: 42.4%
- **Mean IoU**: 0.021 (2.1%) âŒ
- **Per-class IoU**: Only class 0 (background) = 72.7%, all others = 0%

**Root Cause**: **SEVERE CLASS IMBALANCE** - The model learned to predict only the background class because it dominates the dataset (likely 70-90% of all pixels).

## ðŸ“Š What Happened

Looking at your test metrics:
```json
{
    "mean_iou": 0.021396883028475512,
    "per_class_iou": [
        0.7273748744229115,  // Class 0: Background âœ“
        0.0,                  // Class 1: No learning âŒ
        0.0,                  // Class 2: No learning âŒ
        // ... all other 32 classes: 0.0 âŒ
    ]
}
```

The model essentially learned: **"Predict background everywhere"** because:
1. Background pixels are ~70-80% of the dataset
2. Standard CrossEntropyLoss treats all classes equally
3. Model gets rewarded for just predicting the majority class
4. Minority classes (rooms, doors, windows) get ignored

## âœ… Solution: Use Class-Weighted Loss

I've created **3 new files** to fix this:

### 1. `diagnose_model.py` - Diagnostic Tool
Analyzes your dataset to identify class imbalance and calculate proper weights.

**Run this first:**
```bash
python diagnose_model.py
```

**What it does:**
- Scans all training masks
- Counts pixels per class
- Calculates imbalance ratio
- Generates recommended class weights
- Saves results to `class_weights.json`

**Expected output:**
```
Class Distribution:
Class 0: 45,000,000 pixels (75%)  â† Dominates!
Class 1: 2,500,000 pixels (4%)
Class 2: 1,800,000 pixels (3%)
...

Recommended weights saved to class_weights.json
```

### 2. `train_fixed.py` - Fixed Training Script
New training script with class weighting and additional improvements.

**Key differences from original `train.py`:**
- âœ… Automatically calculates class weights
- âœ… Uses weighted CrossEntropyLoss
- âœ… Label smoothing (0.1) for regularization
- âœ… Per-class IoU tracking during training
- âœ… Cosine annealing with warm restarts
- âœ… Lower learning rate (5e-5 vs 1e-4)
- âœ… Saves to separate directory: `models/checkpoints_fixed/`

**Run the fixed training:**
```bash
python train_fixed.py
```

**Expected improvements:**
- **After 20 epochs**: IoU ~0.15-0.25 (multiple classes learning)
- **After 50 epochs**: IoU ~0.40-0.50 (most classes learning)
- **After 100 epochs**: IoU ~0.55-0.70 (good performance)
- **After 150 epochs**: IoU ~0.65-0.80 (excellent performance)

### 3. `FIX_LOW_IOU.md` - This Guide
Comprehensive explanation and troubleshooting.

## ðŸ”§ Step-by-Step Fix

### Step 1: Run Diagnostics
```bash
python diagnose_model.py
```

This will show you:
- Which classes are imbalanced
- Recommended class weights
- Any data quality issues

### Step 2: Train with Fixed Script
```bash
python train_fixed.py
```

**Monitor the training output:**
```
Epoch 1: Active classes: 5/34   â† Starting to learn multiple classes
Epoch 10: Active classes: 15/34  â† Good progress
Epoch 30: Active classes: 25/34  â† Excellent
```

The "Active classes" metric shows how many classes have IoU > 0.01.

### Step 3: Test the Fixed Model
```bash
python test_inference.py
```

Update the checkpoint path in `test_inference.py`:
```python
CHECKPOINT_PATH = 'models/checkpoints_fixed/best_model.pth'
```

**Expected results:**
```
Overall Accuracy: 0.75-0.85
Mean IoU: 0.55-0.70
Per-class IoU: Most classes > 0.3, some > 0.7
```

## ðŸ“ˆ Understanding Class Weights

**How it works:**

1. **Without weights** (current broken model):
   ```python
   loss = CrossEntropyLoss()
   # All classes treated equally
   # Model learns: "Just predict class 0 everywhere"
   ```

2. **With weights** (fixed model):
   ```python
   # Class 0 (background): weight = 0.5  â† Less important
   # Class 15 (bedroom): weight = 5.0    â† More important
   # Class 23 (window): weight = 8.0     â† Very important
   
   loss = CrossEntropyLoss(weight=class_weights)
   # Model forced to learn ALL classes
   ```

The weights are calculated as:
```python
weight[class] = total_pixels / (num_classes * class_pixel_count)
```

This makes the loss proportional to class frequency - rare classes get higher weights.

## ðŸŽ¯ What Changed in train_fixed.py

### 1. Class Weight Calculation
```python
def calculate_class_weights(dataloader, num_classes=34):
    """Automatically calculates weights from training data"""
    # Scans all training masks
    # Counts pixels per class
    # Returns normalized weights
```

### 2. Weighted Loss Function
```python
class_weights = calculate_class_weights(train_loader, 34, device)
criterion = nn.CrossEntropyLoss(
    weight=class_weights,          # â† NEW: Force learning of minority classes
    label_smoothing=0.1            # â† NEW: Prevents overconfidence
)
```

### 3. Per-Class IoU Tracking
```python
def train_epoch_with_class_iou(...):
    # Tracks IoU for EACH class separately
    # Shows which classes are learning
    # Counts "active" classes (IoU > 0.01)
```

### 4. Better Learning Rate Schedule
```python
scheduler = CosineAnnealingWarmRestarts(
    optimizer,
    T_0=20,      # Restart every 20 epochs
    T_mult=2,    # Gradual annealing
    eta_min=1e-6
)
```

### 5. Lower Initial Learning Rate
```python
'learning_rate': 5e-5,  # Was: 1e-4
# Lower LR more stable with class weights
```

## ðŸ“Š Expected Training Progress

### With Original train.py (Broken):
```
Epoch 1:  Loss=3.2, IoU=0.01, Active=1/34  â† Only background
Epoch 10: Loss=2.8, IoU=0.02, Active=1/34  â† Still only background
Epoch 50: Loss=2.1, IoU=0.02, Active=2/34  â† Barely any progress
Epoch 100: Loss=1.8, IoU=0.02, Active=2/34 â† Stuck!
```

### With train_fixed.py (Fixed):
```
Epoch 1:  Loss=3.5, IoU=0.05, Active=8/34   â† Multiple classes!
Epoch 10: Loss=2.2, IoU=0.18, Active=18/34  â† Good progress
Epoch 30: Loss=1.4, IoU=0.42, Active=28/34  â† Most classes learning
Epoch 50: Loss=0.9, IoU=0.58, Active=32/34  â† Excellent
Epoch 100: Loss=0.5, IoU=0.68, Active=34/34 â† All classes active!
```

## ðŸ” Why This Happens (Technical Details)

### The Mathematics

With **unweighted loss**, the gradient for class `c` is:
```
âˆ‡L_c âˆ frequency(c) Ã— error(c)
```

Since background is 75% of pixels:
```
âˆ‡L_background = 0.75 Ã— error_background  â† Large gradient
âˆ‡L_bedroom = 0.02 Ã— error_bedroom        â† Tiny gradient
```

The model updates mostly to reduce background error, ignoring bedrooms.

### With Class Weights

```
âˆ‡L_c âˆ weight(c) Ã— frequency(c) Ã— error(c)

# Adjust weights so all classes have similar gradients:
âˆ‡L_background = 0.5 Ã— 0.75 Ã— error = 0.375 Ã— error
âˆ‡L_bedroom = 15.0 Ã— 0.02 Ã— error = 0.30 Ã— error
```

Now all classes contribute roughly equally to the gradient â†’ model learns all classes!

## ðŸš€ Quick Start (TL;DR)

```bash
# 1. Diagnose the issue
python diagnose_model.py

# 2. Train with fixed script
python train_fixed.py

# 3. Wait ~10-15 hours (or overnight)

# 4. Test the fixed model
# Edit test_inference.py: CHECKPOINT_PATH = 'models/checkpoints_fixed/best_model.pth'
python test_inference.py

# Expected: IoU > 0.55 (much better than 0.02!)
```

## ðŸ†˜ Troubleshooting

### Q: Still getting low IoU after 50 epochs?
**A:** Check the training logs for "Active classes":
- If Active < 10: Class weights might need tuning
- If Active > 20 but IoU low: Need more epochs
- If loss not decreasing: Learning rate too low/high

### Q: Training is slower?
**A:** Yes, because the model now learns all 34 classes instead of just 1. This is expected and necessary.

### Q: Some classes still have 0 IoU?
**A:** Classes with very few samples (<100 pixels total) might need:
- More training epochs (150-200)
- Data augmentation focusing on those classes
- Manual class weight tuning

### Q: Can I use this fix with the original train.py?
**A:** Yes! Just add these lines to `train.py` (around line 330):

```python
# Add this import at top
from collections import Counter

# Replace this line:
criterion = nn.CrossEntropyLoss()

# With this:
def calculate_class_weights(dataloader, num_classes, device):
    class_counts = Counter()
    total_pixels = 0
    for batch in tqdm(dataloader, desc="Calculating class weights"):
        masks = batch['mask'].numpy()
        for mask in masks:
            unique, counts = np.unique(mask, return_counts=True)
            for cls, count in zip(unique, counts):
                class_counts[int(cls)] += int(count)
                total_pixels += int(count)
    
    weights = []
    for i in range(num_classes):
        count = class_counts.get(i, 1)
        weight = min(total_pixels / (num_classes * count), 100.0)
        weights.append(weight)
    
    weights = torch.tensor(weights, dtype=torch.float32)
    weights = weights / weights.sum() * num_classes
    return weights.to(device)

# Calculate weights
class_weights = calculate_class_weights(train_loader, CONFIG['n_classes'], device)
criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)
```

## ðŸ“š Additional Resources

- **Paper**: "Focal Loss for Dense Object Detection" - explains class imbalance in segmentation
- **Tutorial**: Search "weighted cross entropy pytorch segmentation"
- **Alternative**: Use Focal Loss or Dice Loss for severe imbalance

## âœ… Success Criteria

Your model is fixed when you see:
- âœ… Mean IoU > 0.50
- âœ… Active classes > 30/34
- âœ… Per-class IoU distribution: most classes > 0.30
- âœ… Visualizations show diverse predictions (not just background)

## ðŸŽ¯ Summary

**Problem**: Model only learned background class (IoU = 0.02)
**Cause**: Severe class imbalance + unweighted loss
**Solution**: Class-weighted loss function
**Tools**: `diagnose_model.py` + `train_fixed.py`
**Result**: Expected IoU > 0.55 after proper training

---

**Good luck! The fixed training should show dramatic improvements within the first 10-20 epochs.** ðŸš€

*Last Updated: October 28, 2025*
